{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import getpass\n",
    "import base64\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "import operator\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import functools\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.tools import GooglePlacesTool\n",
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "# Set API Keys (temp)\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass(\"Enter your Langchain API key: \")\n",
    "os.environ['TAVILY_API_KEY'] = getpass.getpass(\"Enter your Tavily API key: \")\n",
    "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "os.environ[\"GPLACES_API_KEY\"] = getpass.getpass(\"Enter your Google Places API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search tool\n",
    "search_tool = TavilySearchResults(max_results=3)\n",
    "places_tool = GooglePlacesTool()\n",
    "tools = [search_tool, places_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Execution Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant.\n",
      "\n",
      "=============================\u001b[1m Messages Placeholder \u001b[0m=============================\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{{messages}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"wfh/react-agent-executor\")\n",
    "prompt.pretty_print()\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "agent_executor = create_react_agent(llm, tools, messages_modifier=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Annotated, TypedDict, Union\n",
    "import operator\n",
    "\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    images: List\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(\"user\", state[\"input\"])]\n",
    "# [HumanMessage(content=text_input + image_inputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define Plan class\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "\n",
    "    steps: List[str] = Field(\n",
    "        description=\"different steps to follow, should be in sorted order\"\n",
    "    )\n",
    "\n",
    "# Define planner chain\n",
    "planner_sys_message = \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "If the task requires observing the images, include the string \"(use_images)\" at the end of the task but remain verbose and specific with the full task description. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\"\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", planner_sys_message),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "planner = planner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o\", temperature=0\n",
    ").with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-planning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "# Define Response class\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    response: str\n",
    "\n",
    "# Define Act class\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "# Define replanner chain\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "If the task requires observing the images, include the string \"(use_images)\" at the end of the task but remain verbose and specific with the full task description. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=\"gpt-4o\", temperature=0\n",
    ").with_structured_output(Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# Plan step\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "# Execute step\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    messages = [(\"user\", task_formatted)]\n",
    "    if \"use_images\" in task:\n",
    "        messages.append((\"user\", state[\"images\"]))\n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"messages\": messages}\n",
    "    )\n",
    "    return {\n",
    "        \"past_steps\": (task, agent_response[\"messages\"][-1].content),\n",
    "    }\n",
    "\n",
    "# Replan step\n",
    "async def replan_step(state: PlanExecute):\n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "\n",
    "# Determine if the workflow should end\n",
    "def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Define workflow\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': ['Analyze the streetview images to identify any visible landmarks, street signs, or other identifiable features (use_images).', 'Use the identified features to determine the city and coordinates where the picture was taken.']}\n",
      "{'past_steps': ('Analyze the streetview images to identify any visible landmarks, street signs, or other identifiable features (use_images).', 'In the provided street view image, the following identifiable features are visible:\\n\\n1. **Street Signage**:\\n   - A triangular traffic sign with a bump symbol, indicating a speed bump ahead.\\n   - A sign on the left side of the street with French text, possibly indicating construction or parking information (\"P P A EN FACE\").\\n\\n2. **Street Markings**:\\n   - Bicycle lane markings.\\n   - Pedestrian crossing markings.\\n\\n3. **Construction Site**:\\n   - A construction site with signage that includes the name \"Bouygues,\" which is a well-known French construction company.\\n\\n4. **Buildings and Surroundings**:\\n   - Modern residential buildings.\\n   - Trees without leaves, indicating the image might have been taken during late autumn or winter.\\n\\n5. **Language**:\\n   - The text on the signage is in French, suggesting that this location is in a French-speaking region.\\n\\nBased on these features, it is highly probable that this image was taken in a city in France.')}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', \"To execute step 1, I'll need the identified features of the picture that might include landmarks, notable buildings, or any other distinguishing characteristics that can help identify the city in France. Could you please provide those features or details?\")}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', \"To proceed with step 1 and determine the specific city in France where the picture was taken using identified features, I'll need to know what those identified features are. Could you please provide more details or describe the features visible in the picture? Some useful features might include landmarks, architectural styles, signage, landscapes, and any other distinctive elements.\")}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', 'Please provide the identified features from the picture so that I can help determine the specific city in France where the picture was taken. The identified features can include notable landmarks, buildings, signs, or any other distinctive elements in the image.')}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', 'To execute step 1, I need the identified features from the picture. Could you please provide the features or descriptions that might help in identifying the specific city in France?')}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', 'To execute step 1, I need the identified features of the picture to determine the specific city in France where it was taken. Could you please provide the details or features identified in the picture? This could include landmarks, buildings, signs, or any other notable elements visible in the image.')}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', 'Please provide me with the identified features that you have from the picture. This could include landmarks, buildings, street names, or any other notable characteristics visible in the image.')}\n",
      "{'plan': ['Use the identified features to determine the specific city in France where the picture was taken.', 'Determine the exact coordinates (latitude and longitude) of the identified location in the city.', 'Return the city and coordinates in the specified JSON format.']}\n",
      "{'past_steps': ('Use the identified features to determine the specific city in France where the picture was taken.', 'In order to determine the specific city in France where a picture was taken based on identified features, I would need to know what those identified features are. Could you please provide the features or any other relevant information about the picture? This could include landmarks, signs, buildings, or any other notable characteristics visible in the image.')}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1172\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m# execute tasks, and wait for one to fail or all to finish.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;66;03m# each task is independent from all other concurrent tasks\u001b[39;00m\n\u001b[0;32m-> 1172\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m   1173\u001b[0m     futures,\n\u001b[1;32m   1174\u001b[0m     return_when\u001b[38;5;241m=\u001b[39masyncio\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m   1175\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1176\u001b[0m )\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/asyncio/tasks.py:454\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    453\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _wait(fs, timeout, return_when, loop)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/asyncio/tasks.py:540\u001b[0m, in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m waiter\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m20\u001b[39m}\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Run\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1296\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# wait for all background tasks to finish\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mbg)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/site-packages/langgraph/pregel/retry.py:114\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/site-packages/langchain_core/runnables/base.py:2430\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2429\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2430\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[1;32m   2431\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   2432\u001b[0m             \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[1;32m   2433\u001b[0m             patch_config(\n\u001b[1;32m   2434\u001b[0m                 config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2435\u001b[0m             ),\n\u001b[1;32m   2436\u001b[0m         )\n\u001b[1;32m   2437\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/geo/lib/python3.12/site-packages/langgraph/utils.py:111\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    105\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m     {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config}\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m    113\u001b[0m     )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initial prompts\n",
    "json_prompt = \"Only return a valid json string (RCF8259). Do provide any other commentary. Do not wrap the JSON in markdown such as ```json. Only use the data from the provided content.\"\n",
    "prompt_template = \"\"\"USER: Given a set of streetview images from a vehicle, your task is to determine the \\\n",
    "coordinates from which the picture was taken. It can be anywhere in the world.\n",
    "\n",
    "Return json with the city and coordinates following the below example. {json_prompt}\n",
    "output={{\"city\": \"Orland Park, IL, 60467, USA\", \"latitude\": \"42.0099\", \"longitude\": \"-87.62317\"}}\n",
    "\n",
    "AGENT: output=\"\"\"\n",
    "\n",
    "text_prompt = prompt_template.format(json_prompt=json_prompt)\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "id = \"103\"\n",
    "image_dir = f'./data/{id}/'\n",
    "directions = [\"north\"] #, \"south\", \"east\", \"west\"]\n",
    "image_inputs = []\n",
    "for direction in directions:\n",
    "    base64_img = encode_image(f\"{image_dir}{direction}.png\")\n",
    "    image_input = {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img}\"}}\n",
    "    image_inputs.append(image_input)\n",
    "\n",
    "# Finalize inputs\n",
    "inputs = {\"input\": text_prompt, \"images\": image_inputs}\n",
    "config = {\"recursion_limit\": 20}\n",
    "\n",
    "# Run\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\":\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
